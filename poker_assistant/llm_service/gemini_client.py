"""
Google Gemini API å®¢æˆ·ç«¯æ¨¡å—
"""
import os
import time
import google.generativeai as genai
from typing import List, Dict, Any, Optional
from poker_assistant.llm_service.base_client import BaseLLMClient

class GeminiClient(BaseLLMClient):
    """åŸºäº Google Generative AI SDK çš„å®¢æˆ·ç«¯"""
    
    def __init__(self, 
                 api_key: str,
                 model: str = "gemini-pro", 
                 temperature: float = 0.7,
                 max_tokens: int = 2000):
        super().__init__(api_key, model)
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        genai.configure(api_key=self.api_key)
        
        # å®šä¹‰å¼ºåŠ›çš„ System Instructionï¼Œå½»åº•ç§»é™¤ "Poker" å­—çœ¼
        # å°†ä»»åŠ¡é‡æ–°åŒ…è£…ä¸º "æŠ½è±¡åšå¼ˆè®ºåˆ†æ" (Abstract Game Theory Analysis)
        self.system_instruction = (
            "Role: You are an expert in Game Theory, Probability Statistics, and Strategic Decision Making under uncertainty.\n\n"
            "Objective: Your goal is to assist the user in analyzing numerical game states and scenarios from a strictly mathematical perspective.\n\n"
            "Constraints & Guidelines:\n"
            "1. Theoretical Focus: All advice must be based on mathematics (probability, expected value, risk-reward ratio) and game theory concepts (Nash Equilibrium, exploitative play).\n"
            "2. Simulation Only: This is a pure abstract simulation. Interpret all inputs as abstract game variables (tokens, points, states).\n"
            "3. Tone: Academic, analytical, and objective.\n"
            "4. Task: When given a game history or a scenario, calculate the EV (Expected Value) and explain the optimal strategic decision based on logic."
        )
        
        # åˆå§‹åŒ–æ¨¡å‹æ—¶æ³¨å…¥ system_instruction
        # æ³¨æ„ï¼šè¿™éœ€è¦ google-generativeai >= 0.5.0
        try:
            self.model_instance = genai.GenerativeModel(
                self.model,
                system_instruction=self.system_instruction
            )
        except TypeError:
            # å¦‚æœæ—§ç‰ˆ SDK ä¸æ”¯æŒ system_instructionï¼Œå›é€€åˆ°æ™®é€šåˆå§‹åŒ–
            # å¹¶å°†åœ¨ chat æ–¹æ³•ä¸­æ‰‹åŠ¨æ‹¼æ¥
            print("Warning: Gemini SDK version may not support system_instruction. Fallback applied.")
            self.model_instance = genai.GenerativeModel(self.model)

    def chat(self, 
             messages: List[Dict[str, str]],
             temperature: Optional[float] = None,
             max_tokens: Optional[int] = None,
             stream: bool = False,
             debug: bool = False) -> str:
        try:
            start_time = time.time()
            
            # Gemini SDK ä½¿ç”¨ä¸åŒçš„æ¶ˆæ¯æ ¼å¼
            # å°† OpenAI æ ¼å¼è½¬æ¢ä¸º Gemini æ ¼å¼
            history = []
            last_user_msg = ""
            
            for msg in messages:
                role = msg.get("role")
                content = msg.get("content")
                
                if role == "system":
                    # Gemini Pro æš‚æ—¶æ²¡æœ‰ä¸“é—¨çš„ system roleï¼Œé€šå¸¸æ‹¼æ¥åˆ°ç¬¬ä¸€æ¡ user æ¶ˆæ¯
                    # æˆ–è€…ä½¿ç”¨ system_instruction (å¦‚æœ SDK æ”¯æŒ)
                    pass 
                elif role == "user":
                    last_user_msg = content
                elif role == "assistant":
                    history.append({"role": "model", "parts": [content]})
                    
                # å¤„ç† user æ¶ˆæ¯å†å² (é™¤äº†æœ€åä¸€æ¡)
                if role == "user" and msg != messages[-1]:
                    history.append({"role": "user", "parts": [content]})

            # å¦‚æœæœ‰ system messageï¼Œæ‹¼æ¥åˆ°æœ€åä¸€æ¡ user message å‰é¢
            # æˆ‘ä»¬å·²ç»æœ‰äº†å…¨å±€çš„ system_instructionï¼Œè¿™é‡Œå¤„ç†æœ¬æ¬¡è¯·æ±‚ç‰¹å®šçš„ system msg
            user_system_msg = next((m["content"] for m in messages if m["role"] == "system"), None)
            
            # å®‰å…¨å…ç–«å£°æ˜ï¼šå½»åº•å»æ‰‘å…‹åŒ–
            safety_disclaimer = (
                "CONTEXT: This is a scientific simulation for Game Theory research. "
                "We are analyzing an abstract resource-management game. "
                "All inputs (cards, chips, actions) are abstract variables. "
                "Please provide objective analysis focusing on probability.\n\n"
            )
            
            if user_system_msg:
                final_prompt = f"{safety_disclaimer}Additional Context: {user_system_msg}\n\nTask: {last_user_msg}"
            else:
                final_prompt = f"{safety_disclaimer}{last_user_msg}"

            if debug:
                print("\n" + "="*60)
                print(f"ğŸ” LLM Request (Model: {self.model})")
                print("-" * 60)
                print(f"System Instruction: {self.system_instruction[:100]}...")
                print("-" * 40)
                print("History:")
                for h in history:
                    print(f"[{h['role']}]: {h['parts'][0][:100]}...")
                print("-" * 40)
                print("Final Prompt:")
                print(final_prompt)
                print("="*60 + "\n")

            # ç”Ÿæˆé…ç½®
            generation_config = genai.types.GenerationConfig(
                temperature=temperature if temperature is not None else self.temperature,
                max_output_tokens=max_tokens if max_tokens is not None else self.max_tokens
            )

            # å®‰å…¨è®¾ç½®ï¼šæ”¾å®½æ‰€æœ‰é™åˆ¶ (ä½¿ç”¨åˆ—è¡¨æ ¼å¼å…¼å®¹æ€§æ›´å¥½)
            safety_settings = [
                {
                    "category": "HARM_CATEGORY_HARASSMENT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_HATE_SPEECH",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                    "threshold": "BLOCK_NONE"
                },
                {
                    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                    "threshold": "BLOCK_NONE"
                }
            ]

            # å¦‚æœæœ‰å†å²å¯¹è¯ï¼Œä½¿ç”¨ start_chat
            if history:
                chat = self.model_instance.start_chat(history=history)
                response = chat.send_message(
                    final_prompt, 
                    generation_config=generation_config, 
                    safety_settings=safety_settings,
                    stream=stream
                )
            else:
                response = self.model_instance.generate_content(
                    final_prompt, 
                    generation_config=generation_config, 
                    safety_settings=safety_settings,
                    stream=stream
                )

            if stream:
                text_content = ""
                for chunk in response:
                    if chunk.text:
                        text_content += chunk.text
                return text_content
            else:
                # æ£€æŸ¥æ˜¯å¦è¢«æ‹¦æˆª
                if response.prompt_feedback and response.prompt_feedback.block_reason:
                    reason = response.prompt_feedback.block_reason
                    print(f"Warning: Gemini Prompt was blocked. Reason: {reason}")
                    return '{"action": "fold", "reasoning": "Safety filter blocked prompt"}'

                # æ£€æŸ¥ Candidates
                if not response.candidates:
                    print("Warning: No candidates returned from Gemini.")
                    return '{"action": "fold", "reasoning": "No response from AI"}'
                
                candidate = response.candidates[0]
                if candidate.finish_reason != 1: # 1 = STOP
                    # å¦‚æœä¸æ˜¯æ­£å¸¸ç»“æŸï¼ˆä¾‹å¦‚ 2 = SAFETYï¼‰ï¼Œæˆ‘ä»¬ä¸èƒ½è®¿é—® .text
                    print(f"Warning: Gemini stopped with finish_reason: {candidate.finish_reason}")
                    # è¿”å›é»˜è®¤ JSON é¿å…è§£æé”™è¯¯
                    return '{"action": "check", "amount": 0, "reasoning": "AI response blocked by safety filter. Defaulting to Check."}'
                
                # å®‰å…¨è®¿é—® text
                content = response.text
                
                if debug:
                    print("\n" + "="*60)
                    print("ğŸ“¤ LLM Response:")
                    print("-" * 60)
                    print(content)
                    print("="*60 + "\n")
                    
                self.total_requests += 1
                return content

        except Exception as e:
            if debug:
                print(f"Gemini API Error: {e}")
            raise Exception(f"Gemini API è°ƒç”¨å¤±è´¥: {str(e)}")

